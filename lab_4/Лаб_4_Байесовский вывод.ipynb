{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDjH5VgLMPCa"
   },
   "source": [
    "# Задача 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8T7M9eQMRRn"
   },
   "source": [
    "Пусть $X_1, X_2, \\ldots, X_n$ — выборка из экспоненциального распределения с параметром $\\lambda$. Найти оценку максимального правдоподобия параметра $\\lambda$, сравнить ее с байесовской оценкой (MAP и математическое ожидание апостреорного распределения), подобрав сопряженное распределение. Сравнить полученные байесовские оценки с оценкой MLE. Найти предсказательное распределение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Решение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция плотности вероятности (PDF) для экспоненциального распределения:\n",
    "\n",
    "$$f(x, \\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0,  \\lambda > 0$$\n",
    "\n",
    "Найдём оценку максимального правдоподобия параметра $\\lambda$:\n",
    "\n",
    "$$L(\\lambda, X) = \\prod_{i=1}^n f(x_i, \\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda x_i} = \\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}$$\n",
    "\n",
    "$$\\ln L(\\lambda) = n \\ln \\lambda - \\lambda \\sum_{i=1}^n x_i$$\n",
    "\n",
    "$$\\frac{d l(\\lambda)}{d \\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0$$\n",
    "\n",
    "$$\\hat{\\lambda}_{MLE} = \\frac{n}{\\sum_{i=1}^n x_i} = \\frac{1}{\\bar{X}}$$\n",
    "\n",
    "Оценка MLE — это величина, обратная выборочному среднему.\n",
    "\n",
    "Для экспоненциального сопряженным распределением является Гамма-распределение. Пусть априорное распределение $\\lambda \\sim \\text{Gamma}(\\alpha, \\beta)$, где $\\alpha$ — параметр формы, а $\\beta$ — параметр темпа (rate). Плотность априорного распределения:\n",
    "\n",
    "$$f(\\lambda) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta \\lambda}  , \\quad \\lambda \\geq 0 $$\n",
    "\n",
    "\n",
    "Согласно теореме Байеса:\n",
    "\n",
    "$$P(\\lambda | X) \\propto L(\\lambda, X) \\cdot P(\\lambda)$$\n",
    "\n",
    "Перемножаем Правдоподобие и Априорное распределение:\n",
    "\n",
    "$$P(\\lambda | X) \\propto \\underbrace{(\\lambda^n e^{-\\lambda \\sum x_i})}_{\\text{Правдоподобие}} \\cdot \\underbrace{(\\lambda^{\\alpha-1} e^{-\\beta \\lambda})}_{\\text{Априорное}}$$\n",
    "\n",
    "$$P(\\lambda | X) \\propto \\lambda^{(n+\\alpha)-1} e^{-\\lambda(\\sum x_i + \\beta)}$$\n",
    "\n",
    "Данное выражение имеет ту же самую форму, что и априорное Гамма-распределение с новыми параметрами:\n",
    "- $\\alpha_{new} = \\alpha + n$\n",
    "- $\\beta_{new} = \\beta + \\sum_{i=1}^n x_i$\n",
    "\n",
    "\n",
    "Гамма-распределения с параметрами $(\\alpha, \\beta)$ обладает формулами для среднего и моды:\n",
    "1. Оценка математического ожидания: $E[\\lambda] = \\frac{\\alpha_{new}}{\\beta_{new}}$.\n",
    "\n",
    "$$\\hat{\\lambda}_{mean} = \\frac{\\alpha + n}{\\beta + \\sum x_i}$$\n",
    "\n",
    "2. Оценка MAP это мода распределения (самое вероятное значение): $\\frac{\\alpha_{new} - 1}{\\beta_{new}}$ (при условии, что $\\alpha_{new} > 1$).\n",
    "\n",
    "$$\\hat{\\lambda}_{MAP} = \\frac{\\alpha + n - 1}{\\beta + \\sum x_i}$$\n",
    "\n",
    "Преимущество байесовского подхода проявляется, когда $n$ мало: если у нас нет данных $n=0$, $\\hat{\\lambda}_{mean}$ равна априорному среднему $\\frac{\\alpha}{\\beta}$. Байесовские оценки предотвращают переобучение и экстремальные результаты, которые могут возникнуть при использовании MLE на очень маленьких выборках, сдвигая оценку к априорному среднему. Если $n$ становится очень большим (стремится к бесконечности), то $n$ и $\\sum x_i$ становятся огромными, а $\\alpha$ и $\\beta$ перестают влиять на дробь ${\\Longrightarrow}$ байесовские оценки сходятся к $\\hat{\\lambda}_{MLE}$.\n",
    "\n",
    "\n",
    "**Предсказательное распределение.** \n",
    "\n",
    "Мы получили данные $X$, обновили свое знание о $\\lambda$. Теперь мы хотим предсказать вероятность появления нового наблюдения $x_{new}$. Мы должны учесть все возможные $\\lambda$, взвешенные на их апостериорную вероятность:\n",
    "\n",
    "$$P(x_{new}|X) = \\int_0^\\infty \\underbrace{P(x_{new}|\\lambda)}_{\\text{Экспоненц.}} \\cdot \\underbrace{P(\\lambda|X)}_{\\text{Гамма (апостериорное)}} d\\lambda$$\n",
    "\n",
    "Пусть $\\alpha' = \\alpha_{new}$ и $\\beta' = \\beta_{new}$, тогда:\n",
    "\n",
    "$$P(x_{new}|X) = \\int_0^\\infty (\\lambda e^{-\\lambda x_{new}}) \\cdot \\left( \\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')} \\lambda^{\\alpha'-1} e^{-\\beta' \\lambda} \\right) d\\lambda$$\n",
    "\n",
    "$$P(x_{new}|X) = \\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')} \\int_0^\\infty \\lambda \\cdot \\lambda^{\\alpha'-1} \\cdot e^{-\\lambda x_{new}} \\cdot e^{-\\beta' \\lambda} d\\lambda = \\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')} \\int_0^\\infty \\lambda^{\\alpha'} e^{-\\lambda(x_{new} + \\beta')} d\\lambda$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Можно заметить, что $\\int_0^\\infty \\lambda^{\\alpha'} e^{-\\lambda(x_{new} + \\beta')} d\\lambda$ - это гамма-функция Эйлера, которая определяется как $\\mathrm{\\Gamma}\\left(\\mu\\right)=\\int_{0}^{\\infty}{t^{\\mu-1}e^{-t}dt}$. В нашем случае $\\mu-1 = \\alpha'$ и ${t} = \\lambda(x_{new} + \\beta')$.\n",
    "\n",
    "$$\\int_0^\\infty \\lambda^{\\alpha'} e^{-\\lambda(x_{new} + \\beta')} d\\lambda = \\frac{\\Gamma(\\alpha' + 1)}{(x_{new} + \\beta')^{\\alpha' + 1}}$$\n",
    "\n",
    "Формула предсказательного распределения упрощается, используя свойство $\\Gamma(\\alpha' + 1) = \\alpha' \\Gamma(\\alpha')$:\n",
    "\n",
    "$$P(x_{new} | X) = \\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')} \\cdot \\frac{\\alpha' \\Gamma(\\alpha')}{(x_{new} + \\beta')^{\\alpha' + 1}} = \\frac{\\alpha' \\cdot (\\beta')^{\\alpha'}}{(\\beta' + x_{new})^{\\alpha' + 1}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WBXJuchMhzE"
   },
   "source": [
    "# Задача 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYWArd6bMjkD"
   },
   "source": [
    "**Мультиномиальное распределение**\n",
    "\n",
    "Пусть проводится серия из $n$ испытаний и в результате каждого испытания происходит ровно одно событие из набора $A_1, A_2, \\dots, A_m$, причем вероятности этих событий равны соответственно $\\mathsf{p}_1, \\mathsf{p}_2, \\dots, \\mathsf{p}_m$, причем\n",
    "$$\\sum_{i=1}^{m}\\mathsf{p}_i = 1.$$\n",
    "\n",
    "Тогда совместное распределение величин $X_1, X_2, \\dots, X_m$, где $X_k$ — число наступлений события $A_k$ в серии из $n$ испытаний, задается вероятностями\n",
    "\n",
    "$$\n",
    "\\mathsf{P}\\left(X_1 = n_1, \\dots, X_m = n_m, \\right) = \\frac{n!}{n_1!\\dots n_m!}\\mathsf{p}_1^{n_1}\\dots \\mathsf{p}_m^{n_m},\n",
    "$$\n",
    "\n",
    "где $n_1, n_2, \\dots, n_m$ — произвольный набор целых неотрицательных чисел, таких что\n",
    "\n",
    "$$\\sum_{i=1}^m n_i = n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOvNMoSHMrWR"
   },
   "source": [
    "Произведите байесовский вывод для мультиномиального распределения: найдите апостериорное распределение, используя в качестве сопоряженного распределения к правдоподобию [распределение Дирихле](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%94%D0%B8%D1%80%D0%B8%D1%85%D0%BB%D0%B5), найдите предсказательное распределение. Объясните результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Решение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для Байесовского вывода важна зависимость только от параметра $\\mathbf{p}$, поэтому правдоподобие:\n",
    "\n",
    "$$L(\\mathbf{p}) \\propto \\prod_{k=1}^{m} p_k^{n_k} = p_1^{n_1} \\cdot p_2^{n_2} \\cdot \\dots \\cdot p_m^{n_m}$$\n",
    "\n",
    "Плотность вероятности в распределении Дирихле:\n",
    "\n",
    "$$f(\\mathbf{p}) = \\frac{1}{B(\\alpha)} \\prod_{k=1}^{m} p_k^{\\alpha_k - 1}$$\n",
    "\n",
    "Где:\n",
    "- $p_{k}\\geqslant 0, \\quad \\sum _{i=1}^{m}p_{k}=1, \\quad \\alpha _{k}>0$;\n",
    "- $B(\\alpha) ={\\frac {\\prod \\limits _{i=1}^{K}\\Gamma (\\alpha _{i})}{\\Gamma \\left(\\sum \\limits _{i=1}^{K}\\alpha _{i}\\right)}}$.\n",
    "\n",
    "Используя теорему Байеса:\n",
    "\n",
    "$$P(\\mathbf{p} | X) \\propto \\text{Правдоподобие} \\times \\text{Априорное}$$\n",
    "\n",
    "$$P(\\mathbf{p} | X) \\propto \\left( \\prod_{k=1}^{m} p_k^{n_k} \\right) \\cdot \\left( \\prod_{k=1}^{m} p_k^{\\alpha_k - 1} \\right)$$\n",
    "\n",
    "$$P(\\mathbf{p} | X) \\propto \\prod_{k=1}^{m} p_k^{n_k + \\alpha_k - 1}$$\n",
    "\n",
    "Итого: апостериорное распределение — это распределение Дирихле с параметрами $\\alpha_{new}$:\n",
    "\n",
    "$$\\mathbf{p} | X \\sim \\text{Dir}(\\alpha_1 + n_1, \\ \\alpha_2 + n_2, \\ \\dots, \\ \\alpha_m + n_m)$$\n",
    "\n",
    "**Предсказательное распределение.**\n",
    "\n",
    "Выведем вероятность получить новый набор $\\tilde{x}$ (где сумма $\\sum \\tilde{n}_k = \\tilde{n}$). Формула полной вероятности: \n",
    "\n",
    "$$P(\\tilde{x} | X) = \\int P(\\tilde{x} | \\mathbf{p}) \\cdot P(\\mathbf{p} | X) d\\mathbf{p}$$\n",
    "\n",
    "Обозначим $\\alpha' = \\alpha_{new}$ ($\\alpha_k' = \\alpha_k + n_k$):\n",
    "\n",
    "$$P(\\tilde{x} | X) = \\int \\left( \\frac{\\tilde{n}!}{\\prod \\tilde{n}_k!} \\prod p_k^{\\tilde{n}_k} \\right) \\cdot \\left( \\frac{1}{B(\\alpha')} \\prod p_k^{\\alpha_k' - 1} \\right) d\\mathbf{p}$$\n",
    "\n",
    "$$P(\\tilde{x} | X) = \\frac{\\tilde{n}!}{\\prod \\tilde{n}_k!} \\frac{1}{B(\\alpha')} \\int \\prod_{k=1}^{m} p_k^{\\tilde{n}_k + \\alpha_k' - 1} d\\mathbf{p}$$\n",
    "\n",
    "Интеграл вида $\\int \\prod p_k^{A_k - 1} d\\mathbf{p}$ — это определение Бета-функции $B(A)$, где $A = (\\alpha_1' + \\tilde{n}_1, \\dots, \\alpha_m' + \\tilde{n}_m)$. Значит: \n",
    "\n",
    "$$P(\\tilde{x} | X) = \\frac{\\tilde{n}!}{\\prod_{k=1}^m \\tilde{n}_k!} \\cdot \\frac{B(\\alpha' + \\tilde{n})}{B(\\alpha')}$$\n",
    "\n",
    "Если мы хотим узнать вероятность того, что следующий один бросок даст результат $j$ (то есть $x_{new} = j$): это математическое ожидание параметра $p_j$ в апостериорном распределении.\n",
    "\n",
    "В нашем апостериорном случае:\n",
    "\n",
    "$$P(x_{new} = k | X) = \\frac{\\alpha_k + n_k}{\\sum_{i=1}^m (\\alpha_i + n_i)} = \\frac{\\alpha_k + n_k}{\\sum \\alpha + n}$$\n",
    "\n",
    "Если бы мы бросили монету 3 раза, и выпало 3 \"орла\" ($n_1=3, n_2=0$). MLE скажет, что вероятность \"решки\" равна $0$. То есть решка не выпадет никогда. Это плохой прогноз. Байес (с $\\alpha=1$) скажет, что вероятность \"решки\" равна $\\frac{1 + 0}{2 + 3} = \\frac{1}{5} = 20\\%$. \n",
    "\n",
    "- Числитель $1+0$: мы добавили 1 виртуальную решку;\n",
    "- Знаменатель $2+3$: мы добавили 2 виртуальных броска всего (1 орел, 1 решка). \n",
    "\n",
    "Априорное распределение Дирихле не дает вероятностям стать нулем, даже если событие еще ни разу не случилось в эксперименте."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
